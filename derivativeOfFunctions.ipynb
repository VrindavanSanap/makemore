{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from math import e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224331"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1+e**-x))\n",
    "sigmoid(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04497274992831812\n",
      "0.04517720858176477\n"
     ]
    }
   ],
   "source": [
    "# numerically calculating gradient\n",
    "h=0.01\n",
    "x=3\n",
    "dsdx=(sigmoid(x+h)-sigmoid(x))/h\n",
    "print(dsdx)\n",
    "dsdx=(sigmoid(x+h)-sigmoid(x-h))/(h*2)\n",
    "print(dsdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0452)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(3.0,requires_grad=True)\n",
    "y=torch.sigmoid(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analytically calculating gradient \n",
    "def dsigmoidDx(x):\n",
    "    return (sigmoid(x))*(1-sigmoid(x))\n",
    "dsigmoidDx(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#softmax\n",
    "x=np.array([1,2,3])\n",
    "def softmax(x):\n",
    "    #what is the nice way of writing sigmoid function without spaggeti code\n",
    "    acc=sum(map(lambda x:e**x,x))\n",
    "    \n",
    "    return e**x/acc\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08226163 -0.02212356 -0.06013807]\n",
      "[-0.02208925  0.18530794 -0.16321869]\n",
      "[-0.05979272 -0.16253348  0.2223262 ]\n"
     ]
    }
   ],
   "source": [
    "#numerical gradient of softmax\n",
    "x=np.array([1,2,3])\n",
    "h=0.01\n",
    "for i in range(3):\n",
    "\n",
    "    zeros=np.zeros(3)\n",
    "    zeros[i]=h\n",
    "    print((softmax(x+zeros)-softmax(x))/h)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652], dtype=torch.float64,\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([1,2,3],dtype=float,requires_grad=True)\n",
    "torch.nn.functional.softmax(x,dim=0)\n",
    "# is there any way to do this in pytroch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08192506906499324, -0.022033044520174298, -0.05989202454481893]\n",
      "[-0.022033044520174298, 0.1848364465099787, -0.1628034019898044]\n",
      "[-0.05989202454481893, -0.1628034019898044, 0.2226954265346234]\n"
     ]
    }
   ],
   "source": [
    "#calculating gradients analytically \n",
    "for i in range(3):\n",
    "    a=[]\n",
    "    for j in range(3):\n",
    "        if (i==j):\n",
    "            a.append((softmax(x)[i]*(1-softmax(x)[i])).item())\n",
    "        else:\n",
    "            a.append((-softmax(x)[i]*softmax(x)[j]).item())\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09044131981050807\n",
      "-0.7543457747791082\n",
      "0.6663532031848884\n"
     ]
    }
   ],
   "source": [
    "#softmax + cross entropy\n",
    "x=np.array([1,2,3])\n",
    "y=np.array([0,1,0])\n",
    "def forward(x,y):\n",
    "    o=softmax(x)\n",
    "    # cross entropy\n",
    "    loss=-(np.log(o)*y).sum()\n",
    "    return loss\n",
    "for i in range(3):\n",
    "\n",
    "    zeros=np.zeros(3)\n",
    "    zeros[i]=h\n",
    "    print((forward(x+zeros,y)-forward(x,y))/h)\n",
    "# print(forward(x,y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0900, -0.7553,  0.6652], dtype=torch.float64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([1,2,3],dtype=float,requires_grad=True)\n",
    "y=torch.tensor([0,1,0],dtype=float,requires_grad=True)\n",
    "\n",
    "loss=torch.nn.functional.cross_entropy(x,y)\n",
    "loss.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057317038046, -0.7552715289452023, 0.6652409557748218]\n"
     ]
    }
   ],
   "source": [
    "#calculating analytically\n",
    "x=np.array([1,2,3])\n",
    "y=np.array([0,1,0])\n",
    "o=softmax(x)\n",
    "\n",
    "dldx=[]\n",
    "for i in range(3):\n",
    "    dldx.append(o[i]-y[i])\n",
    "print(dldx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
