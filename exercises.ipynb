{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#\n",
    "# Excersises\n",
    "#\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why not both?\n",
    "# possible trigrams =27*27*27 =19683 (too big to visuallize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By brain is missing some matmul insights so lets do counting model\n",
    "# first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033 15 2\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "url=\"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\n",
    "res=requests.get(url)\n",
    "words=(res.text).splitlines()\n",
    "print(len(words),max(len(w) for w in words),min(len(w) for w in words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars =sorted(list(set((''.join(words)))))\n",
    "# map char to int \n",
    "stoi={s:i+1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"]=0\n",
    "# map int to char \n",
    "itos={i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate trigram frequencies in Matrix Form \n",
    "N=torch.zeros(27,27,27 ,dtype=int)\n",
    "b={}\n",
    "\n",
    "for w in words:\n",
    "    chs=['.']+list(w)+['.']\n",
    "    for ch1,ch2 ,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "        ix1=stoi[ch1]\n",
    "        ix2=stoi[ch2]\n",
    "        ix3=stoi[ch3]\n",
    "        N[ix1][ix2][ix3]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 27, 27])\n"
     ]
    }
   ],
   "source": [
    "# Convert Trigram frequency matrix to probability matrix\n",
    "P=(N+1).float()# adding 1 to prevent probability from being 0 giving infinite loss\n",
    "P=P/P.sum(2,keepdim=True)\n",
    "print(P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7037e-02, 9.8990e-05, 3.8314e-03, 5.9172e-03, 1.9194e-03, 2.4295e-04,\n",
       "        1.4085e-02, 1.5385e-02, 6.4350e-04, 3.5261e-04, 1.2987e-02, 5.3191e-03,\n",
       "        6.2696e-04, 1.2255e-03, 1.1990e-03, 3.8081e-04, 1.3514e-02, 2.6316e-02,\n",
       "        4.4723e-04, 1.9685e-03, 3.3784e-03, 7.3638e-04, 2.0040e-03, 8.4034e-03,\n",
       "        1.5873e-02, 7.2833e-04, 2.9412e-03])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xox.\n",
      "ch.\n",
      ".\n",
      "don.\n",
      "fer.\n",
      "rah.\n",
      "elloufkylah.\n",
      "la.\n",
      "di.\n",
      "be.\n"
     ]
    }
   ],
   "source": [
    "# sampling dumb lang  model basesd on probability matrix \n",
    "for i in range(10):\n",
    "\n",
    "  ix=0\n",
    "  ixx=0\n",
    "  out=[]\n",
    "  while True:\n",
    "    p=P[ixx][ix]\n",
    "    # print(itos[ixx],itos[ix])\n",
    "    # to compare againet completely random \n",
    "    # p=N[ix].float()\n",
    "    # p=p/p.sum()\n",
    "    # p=torch.ones(27)/27\n",
    "    ixx=ix\n",
    "    ix= torch.multinomial(p,num_samples=1,replacement=True).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix==0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log -410414.96875\n",
      "nll 410414.96875\n",
      "LOSS nll/n 2.092747449874878\n"
     ]
    }
   ],
   "source": [
    "# loss calculation for model based on probability materix \n",
    "logLikelihood=0.0\n",
    "nll=0\n",
    "n=0\n",
    "for w in  words:\n",
    "    chs=['.']+list(w)+['.']\n",
    "    for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "        ix1=stoi[ch1]\n",
    "        ix2=stoi[ch2]\n",
    "        ix3=stoi[ch3]\n",
    "        prob= P[ix1][ix2][ix3]\n",
    "        logprob=torch.log(prob)#multiplication of probabilites same as addition of negative log probabilites\n",
    "        logLikelihood+=logprob\n",
    "        n+=1  \n",
    "    nll=-logLikelihood;\n",
    "      \n",
    "print(f\"log {logLikelihood}\")\n",
    "print(f\"nll {nll}\")\n",
    "print(f\"LOSS nll/n {nll/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW THE HARD PART TARGET:ACHIEVE LOWER LOSS THAN 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([196113, 2]), torch.Size([196113]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training set \n",
    "\n",
    "xs,ys=[],[]\n",
    "n=0\n",
    "for w in words:\n",
    "    chs=['.']+list(w)+['.']\n",
    "    for ch1,ch2 ,ch2 in zip(chs,chs[1:],chs[2:]):\n",
    "        ix1=stoi[ch1]\n",
    "        ix2=stoi[ch2]\n",
    "        ix3=stoi[ch2]\n",
    "        # print(ch1,ch2) be carful with prints and for loops long for loops are very hard to stop ,kernel become un alive \n",
    "        xs.append([ix1,ix2])\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs=torch.tensor(xs)\n",
    "ys=torch.tensor(ys)\n",
    "xs.shape,ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING MODELLL\n",
    "W=torch.ones(54,27,requires_grad=True).float()\n",
    "xenc= F.one_hot(xs,num_classes=27).float().view(-1,54)\n",
    "\n",
    "# forward pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0639, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in range(1):\n",
    "    # forward pass\n",
    "    num=ys.nelement()\n",
    "    logits= xenc@W\n",
    "    \n",
    "    counts=logits.exp()\n",
    "    \n",
    "    probs=counts/counts.sum(1,keepdims=True)\n",
    "    # loss calculation\n",
    "    \n",
    "    loss=-probs[torch.arange(num),ys].log().mean() #+0.01*(W**2).mean()#regularization ie smoothing the weights\n",
    "    \n",
    "    # loss=F.cross_entropy(probs,ys)\n",
    "    W.grad=None \n",
    "    loss.backward()\n",
    "    print(k,loss)\n",
    "    # a.append(loss.data)\n",
    "    # update\n",
    "    W.data+=-50*W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([[1,2,3],[1,2,3],[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
