{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\n",
    "# url=\"https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt\"\n",
    "res=requests.get(url)\n",
    "words=(res.text).splitlines()\n",
    "print(len(words),max(len(w) for w in words),min(len(w) for w in words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars =sorted(list(set((''.join(words))))) #get unique characters \n",
    "stoi={s:i+1 for i, s in enumerate(chars)}  # map char to int\n",
    "stoi[\".\"]=0 \n",
    "itos={i:s for s,i in stoi.items()}         # map int to char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate bigram frequencies in Matrix Form \n",
    "N=torch.zeros(28,28,dtype=int)\n",
    "b={}\n",
    "for w in  words:\n",
    "    chs=['.']+list(w)+['.']\n",
    "    for ch1,ch2 in zip(chs,chs[1:]):\n",
    "        ix1=stoi[ch1]\n",
    "        ix2=stoi[ch2]\n",
    "        N[ix1][ix2]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bigram freaquencees\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N,cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr=itos[i]+itos[j]\n",
    "        plt.text(j,i,chstr,ha=\"center\",va='bottom',color=\"gray\")\n",
    "        plt.text(j,i,N[i,j].item(),ha=\"center\",va=\"top\",color=\"gray\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Bigram frequency matrix to probability matrix\n",
    "P=(N+1).float()# adding 1 to prevent probability from being 0 giving infinite loss\n",
    "P=P/P.sum(1,keepdim=True)\n",
    "print(P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling dumb lang  model basesd on probability matrix \n",
    "g=torch.Generator().manual_seed(2147483647)\n",
    "for i in range(20):\n",
    "\n",
    "  ix=0\n",
    "  out=[]\n",
    "  while True:\n",
    "    p=P[ix]   \n",
    "    #to compare againet completely random \n",
    "    # p=N[ix].float()\n",
    "    # p=p/p.sum()\n",
    "    # p=torch.ones(27)/27\n",
    "    ix= torch.multinomial(p,num_samples=1,replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix==0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss calculation for model based on probability materix \n",
    "logLikelihood=0.0\n",
    "nll=0\n",
    "n=0\n",
    "for w in  words:\n",
    "    chs=['.']+list(w)+['.']\n",
    "    for ch1,ch2 in zip(chs,chs[1:]):\n",
    "        ix1=stoi[ch1]\n",
    "        ix2=stoi[ch2]\n",
    "        prob= P[ix1][ix2]\n",
    "        logprob=torch.log(prob)#multiplication of probabilites same as addition of negative log probabilites\n",
    "        logLikelihood+=logprob\n",
    "        n+=1  \n",
    "    nll=-logLikelihood\n",
    "      \n",
    "print(f\"log {logLikelihood}\")\n",
    "print(f\"nll {nll}\")\n",
    "print(f\"LOSS nll/n {nll/n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Till now we were directly using prob matrix with wass calculated directly on the entire datset \n",
    "# Now we will be training a model to do that task instead of explicitly making a matrix\n",
    "\n",
    "#increasing the probability is the same as increasing log(prob) which is the same as decreasing negative log(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training set \n",
    "xs,ys=[],[]\n",
    "n=0\n",
    "for w in words:\n",
    "    chs=['.']+list(w)+['.']\n",
    "    for ch1,ch2 in zip(chs,chs[1:]):\n",
    "        ix1=stoi[ch1]\n",
    "        ix2=stoi[ch2]\n",
    "        # print(ch1,ch2) be carful with prints and for loops long for loops are very hard to stop ,kernel become un alive \n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs=torch.tensor(xs)\n",
    "ys=torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=torch.randn((27,27),requires_grad=True).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi= []\n",
    "\n",
    "xenc=F.one_hot(xs,num_classes=27).float()\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    num=xs.nelement()\n",
    "    logits= xenc@W\n",
    "    print(xenc.shape,W.shape)\n",
    "    counts=logits.exp()\n",
    "    break\n",
    "    probs=counts/counts.sum(1,keepdims=True)\n",
    "    # loss calculation\n",
    "    loss=-probs[torch.arange(num),ys].log().mean() +0.01*(W**2).mean() #regularization ie smoothing the weights\n",
    "    # loss=F.cross_entropy(probs,ys) #better way to do the above without reinventing the wheel\n",
    "\n",
    "    W.grad=None \n",
    "    loss.backward()\n",
    "    print(k,loss)\n",
    "    lossi.append(loss.data)\n",
    "    # update\n",
    "    W.data+=-50*W.grad\n",
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yay almost the same loss as the model with direct Prob matrix as both of the have essentially the same information \n",
    "# Now we will sample the MLP model\n",
    "g=torch.Generator().manual_seed(2147647)\n",
    "for i in range(20):\n",
    "\n",
    "  ix=0\n",
    "  out=[]\n",
    "  while True:\n",
    "    # p=P[ix]   \n",
    "    xenc=F.one_hot(torch.tensor([ix]),num_classes=27).float()\n",
    "    logits=xenc @ W\n",
    "    counts=logits.exp()\n",
    "    p=counts/counts.sum(1,keepdims=True)\n",
    "    ix= torch.multinomial(p,num_samples=1,replacement=True, generator=g).item()\n",
    "    \n",
    "    out.append(itos[ix])\n",
    "    if ix==0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yup almost the same samples as well "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbe58ca63fe33f9eeae9e71d10368d2b4a57f2b1b395836210cc60d362c66949"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
